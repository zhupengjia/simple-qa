{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"/home/pzhu/data/qa/squad2_model\"\n",
    "predict_file = \"data/squad2/dev-v2.0.json\"\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "from pytorch_transformers import XLNetForQuestionAnswering\n",
    "model = XLNetForQuestionAnswering.from_pretrained(checkpoint)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xlnet_qa.squad2_reader import SQuAD2Reader\n",
    "\n",
    "reader = SQuAD2Reader(is_training=False)\n",
    "examples, features, datasets = reader.squad_data(predict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import SequentialSampler, DataLoader\n",
    "from xlnet_qa.utils_squad import RawResultExtended, write_predictions_extended\n",
    "\n",
    "sampler = SequentialSampler(datasets)\n",
    "dataloader = DataLoader(datasets, sampler=sampler, batch_size=1)\n",
    "\n",
    "def to_list(tensor):\n",
    "    return tensor.detach().cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tuple(t.to(device) for t in next(iter(dataloader)))\n",
    "example = examples[data[3].item()]\n",
    "feature = features[data[3].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In what country is Normandy located?\n",
      "['The', 'Normans', '(Norman:', 'Nourmands;', 'French:', 'Normands;', 'Latin:', 'Normanni)', 'were', 'the', 'people', 'who', 'in', 'the', '10th', 'and', '11th', 'centuries', 'gave', 'their', 'name', 'to', 'Normandy,', 'a', 'region', 'in', 'France.', 'They', 'were', 'descended', 'from', 'Norse', '(\"Norman\"', 'comes', 'from', '\"Norseman\")', 'raiders', 'and', 'pirates', 'from', 'Denmark,', 'Iceland', 'and', 'Norway', 'who,', 'under', 'their', 'leader', 'Rollo,', 'agreed', 'to', 'swear', 'fealty', 'to', 'King', 'Charles', 'III', 'of', 'West', 'Francia.', 'Through', 'generations', 'of', 'assimilation', 'and', 'mixing', 'with', 'the', 'native', 'Frankish', 'and', 'Roman-Gaulish', 'populations,', 'their', 'descendants', 'would', 'gradually', 'merge', 'with', 'the', 'Carolingian-based', 'cultures', 'of', 'West', 'Francia.', 'The', 'distinct', 'cultural', 'and', 'ethnic', 'identity', 'of', 'the', 'Normans', 'emerged', 'initially', 'in', 'the', 'first', 'half', 'of', 'the', '10th', 'century,', 'and', 'it', 'continued', 'to', 'evolve', 'over', 'the', 'succeeding', 'centuries.']\n",
      "France\n",
      "26 26\n"
     ]
    }
   ],
   "source": [
    "print(example.question_text)\n",
    "print(example.doc_tokens)\n",
    "print(example.orig_answer_text)\n",
    "print(example.start_position, example.end_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids = data[0],\n",
    "                        attention_mask = data[1],\n",
    "                        token_type_ids = data[2],\n",
    "                        cls_index = data[4],\n",
    "                        p_mask = data[5]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_id = int(feature.unique_id)\n",
    "result = RawResultExtended(unique_id= unique_id,\n",
    "                            start_top_log_probs  = to_list(outputs[0][0]),\n",
    "                            start_top_index      = to_list(outputs[1][0]),\n",
    "                            end_top_log_probs    = to_list(outputs[2][0]),\n",
    "                            end_top_index        = to_list(outputs[3][0]),\n",
    "                            cls_logits           = to_list(outputs[4][0])\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RawResultExtended(unique_id=1000000000, start_top_log_probs=[0.00584795419126749, 0.00584795419126749, 0.005847953725606203, 0.005847953725606203, 0.005847953725606203], start_top_index=[164, 153, 47, 15, 11], end_top_log_probs=[0.005847962573170662, 0.005847966764122248, 0.0058479635044932365, 0.0058479649014770985, 0.005847963970154524, 0.005847962573170662, 0.0058479611761868, 0.0058479635044932365, 0.005847962107509375, 0.0058479611761868, 0.005847962573170662, 0.0058479611761868, 0.0058479635044932365, 0.005847962107509375, 0.0058479611761868, 0.005847959779202938, 0.0058479611761868, 0.005847960710525513, 0.005847962107509375, 0.0058479611761868, 0.005847959779202938, 0.0058479611761868, 0.005847960710525513, 0.005847959313541651, 0.0058479611761868], end_top_index=[56, 46, 64, 46, 46, 46, 51, 47, 52, 75, 42, 47, 46, 45, 52, 52, 52, 54, 51, 144, 47, 50, 52, 16, 69], cls_logits=-0.8711456656455994)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from xlnet_qa.utils_squad import get_final_text, _compute_softmax\n",
    "\n",
    "def write_predictions_extended(example, feature, result, n_best_size,\n",
    "                                max_answer_length, start_n_top, end_n_top, tokenizer):\n",
    "    \"\"\" XLNet write prediction logic (more complex than Bert's).\n",
    "        Write final predictions to the json file and log-odds of null if needed.\n",
    "\n",
    "        Requires utils_squad_evaluate.py\n",
    "    \"\"\"\n",
    "    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"PrelimPrediction\",\n",
    "        [\"start_index\", \"end_index\",\n",
    "        \"start_log_prob\", \"end_log_prob\"])\n",
    "\n",
    "    _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"NbestPrediction\", [\"text\", \"start_log_prob\", \"end_log_prob\"])\n",
    "\n",
    "    prelim_predictions = []\n",
    "    # keep track of the minimum score of null start+end of position 0\n",
    "    score_null = 1000000  # large and positive\n",
    "\n",
    "    cur_null_score = result.cls_logits\n",
    "\n",
    "    # if we could have irrelevant answers, get the min score of irrelevant\n",
    "    score_null = min(score_null, cur_null_score)\n",
    "\n",
    "    for i in range(start_n_top):\n",
    "        for j in range(end_n_top):\n",
    "            start_log_prob = result.start_top_log_probs[i]\n",
    "            start_index = result.start_top_index[i]\n",
    "\n",
    "            j_index = i * end_n_top + j\n",
    "\n",
    "            end_log_prob = result.end_top_log_probs[j_index]\n",
    "            end_index = result.end_top_index[j_index]\n",
    "\n",
    "            # We could hypothetically create invalid predictions, e.g., predict\n",
    "            # that the start of the span is in the question. We throw out all\n",
    "            # invalid predictions.\n",
    "            if start_index >= feature.paragraph_len - 1:\n",
    "                continue\n",
    "            if end_index >= feature.paragraph_len - 1:\n",
    "                continue\n",
    "\n",
    "            if not feature.token_is_max_context.get(start_index, False):\n",
    "                continue\n",
    "            if end_index < start_index:\n",
    "                continue\n",
    "            length = end_index - start_index + 1\n",
    "            if length > max_answer_length:\n",
    "                continue\n",
    "\n",
    "            prelim_predictions.append(\n",
    "                _PrelimPrediction(\n",
    "                    start_index=start_index,\n",
    "                    end_index=end_index,\n",
    "                    start_log_prob=start_log_prob,\n",
    "                    end_log_prob=end_log_prob))\n",
    "\n",
    "    prelim_predictions = sorted(\n",
    "        prelim_predictions,\n",
    "        key=lambda x: (x.start_log_prob + x.end_log_prob),\n",
    "        reverse=True)\n",
    "\n",
    "    seen_predictions = {}\n",
    "    nbest = []\n",
    "    for pred in prelim_predictions:\n",
    "        if len(nbest) >= n_best_size:\n",
    "            break\n",
    "\n",
    "        # XLNet un-tokenizer\n",
    "        # Let's keep it simple for now and see if we need all this later.\n",
    "        # \n",
    "        # tok_start_to_orig_index = feature.tok_start_to_orig_index\n",
    "        # tok_end_to_orig_index = feature.tok_end_to_orig_index\n",
    "        # start_orig_pos = tok_start_to_orig_index[pred.start_index]\n",
    "        # end_orig_pos = tok_end_to_orig_index[pred.end_index]\n",
    "        # paragraph_text = example.paragraph_text\n",
    "        # final_text = paragraph_text[start_orig_pos: end_orig_pos + 1].strip()\n",
    "\n",
    "        # Previously used Bert untokenizer\n",
    "        tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n",
    "        orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
    "        orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
    "        orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n",
    "        tok_text = tokenizer.convert_token_best_sizens_to_string(tok_tokens)\n",
    "\n",
    "        # Clean whitespace\n",
    "        tok_text = tok_text.strip()\n",
    "        tok_text = \" \".join(tok_text.split())\n",
    "        orig_text = \" \".join(orig_tokens)\n",
    "\n",
    "        final_text = get_final_text(tok_text, orig_text, tokenizer.do_lower_case,\n",
    "                                    False)\n",
    "\n",
    "        if final_text in seen_predictions:\n",
    "            continue\n",
    "\n",
    "        seen_predictions[final_text] = True\n",
    "\n",
    "        nbest.append(\n",
    "            _NbestPrediction(\n",
    "                text=final_text,\n",
    "                start_log_prob=pred.start_log_prob,\n",
    "                end_log_prob=pred.end_log_prob))\n",
    "\n",
    "    # In very rare edge cases we could have no valid predictions. So we\n",
    "    # just create a nonce prediction in this case to avoid failure.\n",
    "    if not nbest:\n",
    "        nbest.append(\n",
    "            _NbestPrediction(text=\"\", start_log_prob=-1e6,\n",
    "            end_log_prob=-1e6))\n",
    "\n",
    "    total_scores = []\n",
    "    best_non_null_entry = None\n",
    "    for entry in nbest:\n",
    "        total_scores.append(entry.start_log_prob + entry.end_log_prob)\n",
    "        if not best_non_null_entry:\n",
    "            best_non_null_entry = entry\n",
    "\n",
    "    probs = _compute_softmax(total_scores)\n",
    "\n",
    "    nbest_json = []\n",
    "    for (i, entry) in enumerate(nbest):\n",
    "        output = collections.OrderedDict()\n",
    "        output[\"text\"] = entry.text\n",
    "        output[\"probability\"] = probs[i]\n",
    "        output[\"start_log_prob\"] = entry.start_log_prob\n",
    "        output[\"end_log_prob\"] = entry.end_log_prob\n",
    "        nbest_json.append(output)\n",
    "\n",
    "    assert len(nbest_json) >= 1\n",
    "    assert best_non_null_entry is not None\n",
    "\n",
    "    score_diff = score_null\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(score_diff)\n",
    "    print(best_non_null_entry.text)\n",
    "    print(nbest_json)\n",
    "    return  best_non_null_entry.text, score_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "-0.8711456656455994\n",
      "their\n",
      "[OrderedDict([('text', 'their'), ('probability', 0.33333333354029393), ('start_log_prob', 0.005847953725606203), ('end_log_prob', 0.0058479611761868)]), OrderedDict([('text', 'their name to Normandy, a'), ('probability', 0.33333333354029393), ('start_log_prob', 0.005847953725606203), ('end_log_prob', 0.0058479611761868)]), OrderedDict([('text', 'Normans (Norman:'), ('probability', 0.3333333329194122), ('start_log_prob', 0.005847953725606203), ('end_log_prob', 0.005847959313541651)])]\n"
     ]
    }
   ],
   "source": [
    "write_predictions_extended(example, feature, result, 20, 30,\n",
    "                  model.config.start_n_top, model.config.end_n_top,\n",
    "                  reader.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
